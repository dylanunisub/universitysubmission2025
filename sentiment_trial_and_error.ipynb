{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd81ad57-4448-4c1a-8ea8-53c6ea5799e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install vaderSentiment textblob nltk\n",
    "%pip install datasets\n",
    "%pip install transformers\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "%pip install tensorflow\n",
    "%pip install praw\n",
    "%pip install tf-keras\n",
    "%pip install gensim spacy nltk wordcloud\n",
    "%pip install spacy\n",
    "%pip install nltk \n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16c3f1fa-6c1e-4aa7-acbe-5bf1373e0b5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56370f69-36bf-4aec-b456-efa5d2215ddc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import nltk as nltk\n",
    "import os, time, math\n",
    "import pandas as pd\n",
    "import praw\n",
    "import zipfile\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "from transformers import pipeline\n",
    "from pyspark.sql.functions import col, when, row_number, desc, concat_ws, udf, substring, when, avg\n",
    "from datetime import datetime\n",
    "from pyspark.sql.types import StringType, FloatType, StructType, StructField, ArrayType\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import wordnet\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import brown\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "from tqdm import tqdm \n",
    "from gensim.corpora import Dictionary\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cddd617-7993-4ed6-82f9-f09dccf86a5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"[REDACTED]\",\n",
    "    client_secret=\"[REDACTED]\",\n",
    "    user_agent=\"test_sent\"\n",
    ")\n",
    "\n",
    "SUBREDDITS = [\"BritishProblems\", \"AskUK\", \"UnitedKingdom\", \"UKPolitics\"]\n",
    "\n",
    "# Mix of listings to cover recent + historic + trending\n",
    "STREAMS = {\n",
    "    \"hot\":           {\"fn\": \"hot\",           \"limit\": 500},\n",
    "    \"new\":           {\"fn\": \"new\",           \"limit\": 800},\n",
    "    \"rising\":        {\"fn\": \"rising\",        \"limit\": 200},\n",
    "    \"top_day\":       {\"fn\": \"top\",           \"limit\": 400, \"time_filter\": \"day\"},\n",
    "    \"top_week\":      {\"fn\": \"top\",           \"limit\": 600, \"time_filter\": \"week\"},\n",
    "    \"top_month\":     {\"fn\": \"top\",           \"limit\": 800, \"time_filter\": \"month\"},\n",
    "    \"top_year\":      {\"fn\": \"top\",           \"limit\": 1000,\"time_filter\": \"year\"},\n",
    "    \"top_all\":       {\"fn\": \"top\",           \"limit\": 1000,\"time_filter\": \"all\"},\n",
    "    \"contro_week\":   {\"fn\": \"controversial\", \"limit\": 300, \"time_filter\": \"week\"},\n",
    "    \"contro_year\":   {\"fn\": \"controversial\", \"limit\": 600, \"time_filter\": \"year\"},\n",
    "}\n",
    "\n",
    "# Targeted keyword queries\n",
    "SEARCH_QUERIES = [\n",
    "    \"emigrate OR emigration\",\n",
    "    \"\\\"leave the UK\\\" OR \\\"leaving the UK\\\"\",\n",
    "    \"move abroad OR moving abroad OR relocation\",\n",
    "    \"visa OR work permit OR sponsorship\",\n",
    "    \"cost of living OR housing OR rent OR salary\",\n",
    "    \"NHS OR doctor OR nurse OR healthcare job\",\n",
    "]\n",
    "SEARCH_LIMIT_PER_QUERY = 300  # per subreddit per query\n",
    "\n",
    "MIN_DATE = \"2016-01-01\"  \n",
    "MIN_TS = int(datetime.strptime(MIN_DATE, \"%Y-%m-%d\").timestamp()) if MIN_DATE else None\n",
    "\n",
    "SLEEP = 0.6 \n",
    "\n",
    "def to_row(sub, sr_name):\n",
    "    dt = datetime.utcfromtimestamp(sub.created_utc)\n",
    "    return {\n",
    "        \"id\": sub.id,\n",
    "        \"subreddit\": sr_name,\n",
    "        \"title\": sub.title or \"\",\n",
    "        \"selftext\": sub.selftext or \"\",\n",
    "        \"author\": str(sub.author) if sub.author else \"[deleted]\",\n",
    "        \"score\": sub.score,\n",
    "        \"num_comments\": sub.num_comments,\n",
    "        \"created_utc\": sub.created_utc,\n",
    "        \"created_dt\": dt.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"permalink\": f\"https://www.reddit.com{sub.permalink}\",\n",
    "        \"url\": sub.url,\n",
    "        \"flair\": sub.link_flair_text,\n",
    "        \"over_18\": sub.over_18,\n",
    "        \"locked\": sub.locked,\n",
    "        \"distinguished\": sub.distinguished\n",
    "    }\n",
    "\n",
    "def add_batch(iterable, sr_name, seen, out):\n",
    "    new_count = 0\n",
    "    for sub in iterable:\n",
    "        if MIN_TS and sub.created_utc < MIN_TS:\n",
    "            # skip older than desired window\n",
    "            continue\n",
    "        if sub.id in seen:\n",
    "            continue\n",
    "        seen.add(sub.id)\n",
    "        out.append(to_row(sub, sr_name))\n",
    "        new_count += 1\n",
    "    return new_count\n",
    "\n",
    "\n",
    "all_rows, seen_ids = [], set()\n",
    "\n",
    "for sr in SUBREDDITS:\n",
    "    subreddit = reddit.subreddit(sr)\n",
    "    print(f\"\\n\uD83D\uDCE5 r/{sr}\")\n",
    "\n",
    "    # 1) Listings (hot/new/top/rising/controversial)\n",
    "    for name, cfg in STREAMS.items():\n",
    "        try:\n",
    "            fetch_fn = getattr(subreddit, cfg[\"fn\"])\n",
    "            kwargs = {\"limit\": cfg[\"limit\"]}\n",
    "            if \"time_filter\" in cfg:\n",
    "                kwargs[\"time_filter\"] = cfg[\"time_filter\"]\n",
    "            batch = fetch_fn(**kwargs)\n",
    "            added = add_batch(batch, sr, seen_ids, all_rows)\n",
    "            print(f\"  • {name:<12} +{added} (limit={cfg['limit']})\")\n",
    "            time.sleep(SLEEP)\n",
    "        except Exception as e:\n",
    "            print(f\"  • {name:<12} skipped: {e}\")\n",
    "\n",
    "    # 2) Searches (broad coverage beyond listings)\n",
    "    for q in SEARCH_QUERIES:\n",
    "        try:\n",
    "            batch = subreddit.search(q, sort=\"relevance\", time_filter=\"all\", limit=SEARCH_LIMIT_PER_QUERY)\n",
    "            added = add_batch(batch, sr, seen_ids, all_rows)\n",
    "            print(f\"  • search [{q[:28]}...] +{added} (limit={SEARCH_LIMIT_PER_QUERY})\")\n",
    "            time.sleep(SLEEP)\n",
    "        except Exception as e:\n",
    "            print(f\"  • search [{q[:28]}...] skipped: {e}\")\n",
    "\n",
    "df = pd.DataFrame(all_rows)\n",
    "\n",
    "# Drop NSFW or locked posts if desired\n",
    "df = df[(~df[\"over_18\"]) & (~df[\"locked\"])]\n",
    "\n",
    "# Remove truly empty content\n",
    "df = df[(df[\"title\"].str.strip() != \"\") | (df[\"selftext\"].str.strip() != \"\")]\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "512a71fa-63e3-4c89-946b-ee98384a0eb4",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{\"permalink\":{\"format\":{\"preset\":\"string-preset-url\"}},\"url\":{\"format\":{\"preset\":\"string-preset-url\"}}}},\"syncTimestamp\":1756314288953}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "spk_reddit_dir = spark.createDataFrame(df)\n",
    "spark_reddit_df = spk_reddit_dir.withColumn(\"concatenated_text\", concat_ws(\" \", col(\"title\"), col(\"selftext\")))\n",
    "display(spark_reddit_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a538d22-72b1-43ec-8062-cf76fd2ae77e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Initialize VADER sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to get VADER sentiment scores (for short texts like comments)\n",
    "def get_vader_sentiment(text):\n",
    "    score = analyzer.polarity_scores(str(text))[\"compound\"]\n",
    "    return \"Positive\" if score > 0.05 else \"Negative\" if score < -0.05 else \"Neutral\"\n",
    "\n",
    "# Function to get TextBlob sentiment scores (for longer texts like posts)\n",
    "def get_textblob_sentiment(text):\n",
    "    score = TextBlob(str(text)).sentiment.polarity\n",
    "    return \"Positive\" if score > 0.05 else \"Negative\" if score < -0.05 else \"Neutral\"\n",
    "\n",
    "\n",
    "# Register UDF\n",
    "get_textblob_sentiment_udf = udf(get_textblob_sentiment, StringType())\n",
    "\n",
    "# Apply sentiment analysis to posts\n",
    "spark_reddit_df = spark_reddit_df.withColumn(\"sentiment\", get_textblob_sentiment_udf(spark_reddit_df[\"concatenated_text\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e2e1d29-a845-4921-852c-03b9222a43c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load a sentiment-analysis pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "\n",
    "# Run sentiment\n",
    "result = classifier(\"It’s a disgrace - Labour MP slammed for charging taxpayers £900 ‘pet rent’ for dog to live with her in London home\")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "425608ea-89f0-4910-a946-180580fcd171",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_sentiment(text):\n",
    "    if not text:\n",
    "        return (\"Neutral\", 0.0)\n",
    "    try:\n",
    "        result = sentiment_model(text[:512])[0]  # truncate to 512 tokens\n",
    "        label = label_map.get(result['label'], result['label'])\n",
    "        score = float(result['score'])\n",
    "        return (label, score)\n",
    "    except Exception:\n",
    "        return (\"Neutral\", 0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a7d7efd-20f3-4d6b-87d1-a12324f1189d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the sentiment classifier\n",
    "sentiment_model = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "\n",
    "# Label map (optional, to convert LABEL_0 → Negative, etc.)\n",
    "label_map = {\n",
    "    \"LABEL_0\": \"Negative\",\n",
    "    \"LABEL_1\": \"Neutral\",\n",
    "    \"LABEL_2\": \"Positive\"\n",
    "}\n",
    "\n",
    "# Define schema for struct return type\n",
    "schema = StructType([\n",
    "    StructField(\"sentiment_label\", StringType(), True),\n",
    "    StructField(\"sentiment_score\", FloatType(), True)\n",
    "])\n",
    "\n",
    "# Create UDF\n",
    "sentiment_udf = udf(get_sentiment, schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f19619ad-e1a4-42e0-b022-55c461f8f161",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Apply the UDF\n",
    "df_with_sentiment = spark_reddit_df.withColumn(\"sentiment_struct\", sentiment_udf(\"concatenated_text\"))\n",
    "\n",
    "# Split the struct into separate columns\n",
    "df_with_sentiment = df_with_sentiment \\\n",
    "    .withColumn(\"sentiment_label\", df_with_sentiment[\"sentiment_struct.sentiment_label\"]) \\\n",
    "    .withColumn(\"sentiment_score\", df_with_sentiment[\"sentiment_struct.sentiment_score\"]) \\\n",
    "    .drop(\"sentiment_struct\", \"upvotes\", \"comments\", \"title\", \"text\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d02d01b-d642-4ea6-9fd1-5f9f783ba01d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# If column is string\n",
    "df_with_sentiment = df_with_sentiment.withColumn(\"year\", substring(col(\"created_dt\"), 1, 4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b9c3f0a-202b-4e78-b2ac-3054ba717a8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "columns_to_drop = ['created_dt', 'sentiment']\n",
    "df_with_sentiment = df_with_sentiment.drop(*columns_to_drop)\n",
    "df_with_sentiment.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2c51337-0b97-4c0a-8ef5-0e715138ff7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_with_sentiment = df_with_sentiment.withColumn(\n",
    "    \"sentiment_score\",\n",
    "    when(df_with_sentiment.sentiment_label == \"Positive\", 1)\n",
    "    .when(df_with_sentiment.sentiment_label == \"Negative\", -1)\n",
    "    .otherwise(0)  # optional: treat neutral or unknown as 0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21e10bbf-c05e-480a-a228-df0f21109e3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "yearly_sentiment = df_with_sentiment.groupBy(\"year\").agg(avg(\"sentiment_score\").alias(\"avg_sentiment\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5abcd0c-a4bc-4177-9889-20ed17acbce5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "yearly_sentiment = yearly_sentiment.withColumn(\n",
    "    \"year\",\n",
    "    col(\"year\").cast(\"int\")\n",
    ")\n",
    "\n",
    "yearly_sentiment_pd = yearly_sentiment.orderBy(\"year\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef923d0f-6e9e-4ee6-8aaf-26357bbdc734",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Preprocess Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a9f6829-0135-46cb-b48c-e0d59d41bdf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "nltk.download('wordnet', download_dir=\"/dbfs/tmp/nltk_data/\")\n",
    "\n",
    "dbutils.fs.ls('dbfs:/tmp/nltk_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3678f46-b1cb-4913-a812-b7ab5f7459e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def unzip_if_needed(zip_path, extract_to):\n",
    "    if not os.path.exists(extract_to):\n",
    "        os.makedirs(extract_to, exist_ok=True)\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_to)\n",
    "        print(f\"✅ Unzipped {zip_path} to {extract_to}\")\n",
    "    else:\n",
    "        print(f\"ℹ️ Already extracted: {extract_to}\")\n",
    "\n",
    "unzip_if_needed(\"/dbfs/tmp/nltk_data/corpora/wordnet.zip\", \"/dbfs/tmp/nltk_data/corpora/wordnet\")\n",
    "unzip_if_needed(\"/dbfs/tmp/nltk_data/corpora/omw-1.4.zip\", \"/dbfs/tmp/nltk_data/corpora/omw-1.4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0524c5be-2536-412b-947d-75d03faa2830",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.put('/dbfs/databricks/scripts/nltk-install.sh', \"\"\" \n",
    "#!/bin/bash\n",
    "pip install nltk\"\"\", True)\n",
    "\n",
    "dbutils.fs.head('/dbfs/databricks/scripts/nltk-install.sh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62b3ee37-0bed-406b-8c78-60f2632e2942",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# nltk.download('all', download_dir=\"/dbfs/databricks/nltk_data/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90507603-acfc-4481-bb3d-e52624da09f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "nltk.data.path.append(\"/dbfs/tmp/nltk_data/wordnet/wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c849f4b3-5005-4175-809e-50cdc3040b2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "nltk.data.path.append(\"/dbfs/tmp/nltk_data\")\n",
    "print(wordnet.synsets(\"dog\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29745bae-f10e-4d3c-9944-ea80470029a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Set random seed\n",
    "np.random.seed(400)\n",
    "\n",
    "# Append NLTK data path on the driver (good practice for driver operations too)\n",
    "nltk.data.path.append(\"/dbfs/tmp/nltk_data\")\n",
    "\n",
    "# Load and broadcast stopwords (only plain objects should be broadcasted)\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "sc = spark.sparkContext\n",
    "broadcast_stopwords = sc.broadcast(STOPWORDS)\n",
    "\n",
    "# ✅ Updated Preprocessing Function with Internal nltk.data.path\n",
    "def preprocess(text):\n",
    "    import nltk\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    from gensim.utils import simple_preprocess\n",
    "\n",
    "    # Set nltk data path on worker\n",
    "    nltk.data.path.append(\"/dbfs/tmp/nltk_data\")\n",
    "\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    stopwords_set = broadcast_stopwords.value\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    result = [\n",
    "        lemmatizer.lemmatize(token, 'v')\n",
    "        for token in simple_preprocess(text)\n",
    "        if token not in stopwords_set\n",
    "    ]\n",
    "    return result\n",
    "\n",
    "# Register as UDF\n",
    "preprocess_udf = udf(preprocess, ArrayType(StringType()))\n",
    "\n",
    "# Function to apply to a column\n",
    "def clean_text_column(df, input_col, output_col=\"cleaned_text\"):\n",
    "    return df.withColumn(output_col, preprocess_udf(df[input_col]))\n",
    "\n",
    "# Example usage\n",
    "cleaned_df = clean_text_column(df_with_sentiment, \"concatenated_text\")\n",
    "display(cleaned_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8767be97-6264-44bf-a459-3fd5e344ddcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41d3ab00-f190-4121-b39a-8a80d4ba3494",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "nltk.download('brown')  # download Brown corpus\n",
    "nltk.download('universal_tagset')   # download the POS tags data\n",
    "from nltk.corpus import brown\n",
    "nltk_data = list(brown.tagged_sents(tagset='universal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93dce6af-dafb-4292-9977-5e3e38e990bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "train_set, test_set = train_test_split(\n",
    "    nltk_data,\n",
    "    train_size=0.80,  # use 80% as the training data\n",
    "    test_size=0.20,\n",
    "    random_state=101\n",
    ")\n",
    "print(f'Number of training sentences: {len(train_set)}')\n",
    "print(f'Number of test sentences: {len(test_set)}')\n",
    "\n",
    "# Separate the labels from the text\n",
    "train_toks = []  # each item in the list is a list of tokens in a document\n",
    "train_tags = []  # each item in the list is a list of corresponding tags\n",
    "for tagged_sentence in train_set:\n",
    "    sentence_toks = []\n",
    "    sentence_tags = []\n",
    "    for token, tag in tagged_sentence:\n",
    "        sentence_toks.append(token)\n",
    "        sentence_tags.append(tag)\n",
    "\n",
    "    train_toks.append(sentence_toks)\n",
    "    train_tags.append(sentence_tags)\n",
    "\n",
    "test_toks = []\n",
    "test_tags = []\n",
    "for tagged_sentence in test_set:\n",
    "    sentence_toks = []\n",
    "    sentence_tags = []\n",
    "    for token, tag in tagged_sentence:\n",
    "        sentence_toks.append(token)\n",
    "        sentence_tags.append(tag)\n",
    "    test_toks.append(sentence_toks)\n",
    "    test_tags.append(sentence_tags)\n",
    "\n",
    "print(f'Number of training sentences in train_toks: {len(train_toks)}')\n",
    "print(f'Number of test sentences in test_toks: {len(test_toks)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0927173f-40aa-4105-a52a-04771a63e61f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Convert the tokens to IDs in a vocabulary ready for input to our models\n",
    "dictionary = Dictionary(train_toks + test_toks)\n",
    "\n",
    "train_toks_encoded = [dictionary.doc2idx(sent) for sent in train_toks]\n",
    "test_toks_encoded = [dictionary.doc2idx(sent) for sent in test_toks]\n",
    "print(f'Example sentence: {train_toks_encoded[3]}')\n",
    "\n",
    "V = len(dictionary.values())  # vocabulary\n",
    "print(f'Size of vocabulary is {V}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef49ce24-36f9-4379-872e-e1fc5b87aa4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Convert the tags from their names to numbers\n",
    "tag_encoder = LabelEncoder()\n",
    "tag_encoder.fit([tag for sentence in train_tags for tag in sentence])\n",
    "train_tags_encoded = [tag_encoder.transform(sentence) for sentence in train_tags]\n",
    "test_tags_encoded = [tag_encoder.transform(sentence) for sentence in test_tags]\n",
    "\n",
    "num_tags = len(tag_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8079712-72a3-4b44-9c86-26228f3efe1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tag_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a9433e1-8c53-4ec5-b42c-02e627f7bc5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "transitions = np.zeros((num_tags, num_tags))\n",
    "start_states = np.zeros(num_tags)\n",
    "\n",
    "for sentence_tags in tqdm(train_tags_encoded):\n",
    "    for i, tag in enumerate(sentence_tags):\n",
    "        if i==0:\n",
    "            start_states[tag] += 1\n",
    "            continue\n",
    "        ### WRITE YOUR OWN CODE HERE\n",
    "        transitions[sentence_tags[i-1], tag] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05b5ba37-6754-4b92-baf5-5f9994795f8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE\n",
    "transitions /= np.sum(transitions, 1)[:, None]\n",
    "start_states /= np.sum(start_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2213f055-b469-47aa-8743-61a7f49bd5b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "observations = np.zeros((num_tags, V))\n",
    "\n",
    "for i, sentence_toks in tqdm(enumerate(train_toks_encoded)):\n",
    "    sentence_tags = train_tags_encoded[i]\n",
    "    for j, tok in enumerate(sentence_toks):\n",
    "        tag = sentence_tags[j]\n",
    "        # WRITE YOUR OWN CODE HERE\n",
    "        observations[tag, tok] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c3c5854-80e9-4111-a444-bd1b1fd88433",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def viterbi(observed_seq, num_tags, start_probs, transition_probs, observation_probs):\n",
    "    eps = 1e-7\n",
    "\n",
    "    num_obs = len(observed_seq)\n",
    "\n",
    "    # Initialise the V and backpointers\n",
    "    V = np.zeros((num_obs, num_tags))\n",
    "    backpointer = np.zeros((num_obs, num_tags))\n",
    "\n",
    "    # For the first data point in the sequence:\n",
    "    V[0, :] = start_probs * observation_probs[:, observed_seq[0]]\n",
    "\n",
    "    # Run Viterbi forward for t > 0\n",
    "    for t in range(1, num_obs):\n",
    "\n",
    "        for state in range(num_tags):\n",
    "            # probabilities for all the sequences leading to this state at time t\n",
    "            seq_prob = V[t-1, :] * transition_probs[:, state]\n",
    "\n",
    "            # Choose the most likely sequence\n",
    "            max_seq_prob = np.max(seq_prob)\n",
    "            best_previous_state = np.argmax(seq_prob)\n",
    "\n",
    "            # Calculate the probability of the most likely sequence leading to this state at time t, including the current observation.\n",
    "            # Add eps to help with numerical issues.\n",
    "            V[t, state] = (max_seq_prob + eps) * (observation_probs[state, observed_seq[t]] + eps)\n",
    "\n",
    "            backpointer[t, state] = best_previous_state\n",
    "\n",
    "    t = num_obs - 1\n",
    "\n",
    "    # Initialise the sequence of predicted states\n",
    "    state_seq = np.zeros(num_obs, dtype=int)\n",
    "\n",
    "    # Get the most likely final state:\n",
    "    state_seq[t] = np.argmax(V[t, :])\n",
    "\n",
    "    # Backtrack until the first observation\n",
    "    for t in range(len(observed_seq)-1, 0, -1):\n",
    "        state_seq[t-1] = backpointer[t, state_seq[t]]\n",
    "\n",
    "    return state_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff413ec8-9ead-499a-9977-d075302365d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for sentence in tqdm(test_toks_encoded):\n",
    "    # WRITE YOUR OWN CODE HERE\n",
    "    predictions.append(viterbi(sentence, num_tags, start_states, transitions, observations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "610d4b4f-4277-4e95-9cee-2d3f55c77c1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert the sequence of tag IDs to tag names\n",
    "predicted_tags = []\n",
    "for sequence in tqdm(predictions):\n",
    "    predicted_tags.append(tag_encoder.inverse_transform(sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f69f3f38-6990-4c68-aefe-61e81d274ae1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# print some examples:\n",
    "examples = [2, 334, 4983, 2389]\n",
    "for eg in examples:\n",
    "    print(f'Tokens:      {test_toks[eg]}')\n",
    "    print(f'Gold tag:    {test_tags[eg]}')\n",
    "    print(f'Predictions: {predicted_tags[eg]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "579ef36b-e9ab-4ec4-af5b-59dd1db7bdbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# compute accuracy\n",
    "\n",
    "all_predictions = [tag for sentence in predictions for tag in sentence]\n",
    "all_targets = [tag for sentence in test_tags_encoded for tag in sentence]\n",
    "\n",
    "acc = accuracy_score(all_targets, all_predictions)\n",
    "print(f'Accuracy = {acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ce57ec1-37af-45e1-a22a-d08415adbec1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_name = \"dslim/bert-base-NER\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "# Sample input sentence\n",
    "example = \"Hugging Face Inc. is a company based in New York City.\"\n",
    "\n",
    "# Run inference\n",
    "ner_results = nlp(example)\n",
    "\n",
    "for entity in ner_results:\n",
    "    print(entity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f367e6d0-62ba-4b48-80f9-de540861ae55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "# Load the pretrained NER pipeline with aggregation\n",
    "model_name = \"dslim/bert-base-NER\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "# Function to replace named entities with joined entities\n",
    "def replace_named_entities(text):\n",
    "    entities = nlp(text)\n",
    "    \n",
    "    # Sort entities in reverse order to avoid offset issues during replacement\n",
    "    entities = sorted(entities, key=lambda x: x['start'], reverse=True)\n",
    "    \n",
    "    for entity in entities:\n",
    "        entity_text = entity['word']\n",
    "        joined_entity = re.sub(r'\\s+', '_', entity_text.strip().lower())\n",
    "        \n",
    "        # Replace entity in original text\n",
    "        start, end = entity['start'], entity['end']\n",
    "        text = text[:start] + joined_entity + text[end:]\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Register the function as a UDF\n",
    "replace_named_entities_udf = udf(replace_named_entities, StringType())\n",
    "\n",
    "# Apply the UDF to the DataFrame\n",
    "total_df = cleaned_df.withColumn('processed_text', replace_named_entities_udf(cleaned_df['concatenated_text']))\n",
    "\n",
    "# Check results\n",
    "display(total_df.select('concatenated_text', 'processed_text').limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee1c1b4d-36cd-4c39-b3b0-558684638a47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "total_df.createOrReplaceTempView(\"temp_view_total_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be217688-16d2-4915-a721-0370c69b1a82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 409432246209605,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "20250508 Sentiment Over Time (Total Reddit)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}