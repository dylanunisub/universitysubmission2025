{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "936968bf-5c10-488d-a9ce-e56b9bf10f63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install vaderSentiment textblob nltk\n",
    "%pip install datasets\n",
    "%pip install transformers\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "%pip install tensorflow\n",
    "%pip install praw\n",
    "%pip install tf-keras\n",
    "%pip install gensim spacy nltk wordcloud\n",
    "%pip install spacy\n",
    "%pip install inflect\n",
    "%pip install nltk\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f37ad71-4870-4208-a476-53679dfdacbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install youtube-comment-downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c75fc3e-4c55-41d6-950d-30800ef99475",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from youtube_comment_downloader import YoutubeCommentDownloader\n",
    "import pandas as pd\n",
    "\n",
    "downloader = YoutubeCommentDownloader()\n",
    "\n",
    "urls = [\n",
    "    'https://www.youtube.com/watch?v=8r6FyCMbqow',\n",
    "    'https://www.youtube.com/watch?v=oPD3KR_PtfY',\n",
    "    'https://www.youtube.com/watch?v=QrCqsYqbuAc',\n",
    "    'https://www.youtube.com/watch?v=-Wr-RuoV0pU&ab_channel=BALR',\n",
    "    'https://www.youtube.com/watch?v=nHj8LrOmAgE&ab_channel=ChrisWilliamson',\n",
    "    'https://www.youtube.com/watch?v=mKs0Uyj0q-w&ab_channel=HowMoneyWorks',\n",
    "    'https://www.youtube.com/watch?v=mKs0Uyj0q-w&ab_channel=HowMoneyWorks'\n",
    "]\n",
    "\n",
    "comment_list = []\n",
    "for url in urls:\n",
    "    comments = downloader.get_comments_from_url(url)\n",
    "    for comment in comments:\n",
    "        comment_list.append(comment)\n",
    "\n",
    "df = pd.DataFrame(comment_list)\n",
    "\n",
    "df = df[['text', 'votes']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c1c2c6d-fe8b-4245-9cef-a36a5ed8d1b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# spark_df = spark.createDataFrame(df)\n",
    "# spark_df.createOrReplaceTempView(\"youtube_comments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c251b0d5-55cf-4ba1-9575-cb7eb96662c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "youtube_comments_df = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d665a78-f9fb-41b8-8b2e-fc61b0b40bb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51e7b0d6-0869-4a6d-a49b-fa0e6bf8134a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f052153-a51e-4ff6-a030-7a9065000c96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import inflect\n",
    "import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import inflect\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.utils import simple_preprocess\n",
    "from wordcloud import STOPWORDS as wc_stopwords\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d57c7007-1250-415d-89d5-1aff8e6cbc7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Build a cleaner custom stopword set (deduped)\n",
    "custom_stopwords = set(wc_stopwords).union({\n",
    "    \"uk\",\"like\",\"would\",\"also\",\"getting\",\"got\",\"get\",\"go\",\n",
    "    \"ll\",\"ve\",\"re\",\"don\",\"didn\",\"isn\",\"wasn\",\"weren\",\"day\",\n",
    "    \"now\",\"ask\",\"want\",\"know\",\"think\",\"say\",\"said\",\"saying\",\"says\",\"bn\",\"email\",\n",
    "    \"new\",\"year\",\"person\",\"people\",\"try\",\"look\",\"tell\",\"thing\",\"good\",\"use\",\"will\",\n",
    "    \"see\",\"back\",\"come\",\"one\",\"really\",\"i_am\",\"i_don\",\"take\",\"put\", \"oh\",\n",
    "    \"the_uk\",\"feel\",\"make\",\"today\",\"still\",\"m_not\",\"due_to\",\"even\", \"wa\", \"thi\", \"ha\",\n",
    "    \"seem\", \"we_re\", \"do_you\", \"etc\", \"https_www\", \"hi\", \"let\", \"would_be\",\n",
    "    \"we_re\", \"hi\", \"do_you\", \"seem\", \"etc\", \"would_be\", \"bro\", \"if_you\", \"lol\", \"me\", \"im\",\n",
    "    \"I\"\n",
    "})\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "p = inflect.engine()\n",
    "\n",
    "# Train bigrams on LOWER-CASED, tokenized text (better quality than .split())\n",
    "sentences = df[\"text\"].astype(str).apply(lambda x: simple_preprocess(x))\n",
    "bigram = Phrases(sentences, min_count=5, threshold=10)\n",
    "bigram_model = Phraser(bigram)\n",
    "\n",
    "# Apply bigrams\n",
    "df[\"bigram_text\"] = sentences.apply(lambda toks: \" \".join(bigram_model[toks]))\n",
    "\n",
    "def preprocess(text: str):\n",
    "    tokens = simple_preprocess(text)  # lower, deaccent, min_len=2 by default\n",
    "    cleaned = []\n",
    "    for t in tokens:\n",
    "        # normalize first\n",
    "        t_norm = p.singular_noun(t) or t      \n",
    "        t_norm = lemmatizer.lemmatize(t_norm, 'n')\n",
    "        t_norm = lemmatizer.lemmatize(t_norm, 'v')\n",
    "        # THEN filter\n",
    "        if t_norm not in custom_stopwords:\n",
    "            cleaned.append(t_norm)\n",
    "    return cleaned\n",
    "\n",
    "df[\"processed_text_new\"] = df[\"bigram_text\"].apply(preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a343a9d-fb8b-437c-b0c4-6cb01d1bfcf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "texts = df[\"processed_text_new\"]         # tokenized, preprocessed list-of-lists\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(t) for t in texts]\n",
    "\n",
    "K_RANGE = range(4, 13)   \n",
    "RANDOM_STATE = 42\n",
    "PASSES = 10             \n",
    "ALPHA = 'symmetric'      \n",
    "ETA = 'symmetric'\n",
    "\n",
    "def fit_lda(k):\n",
    "    model = LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=dictionary,\n",
    "        num_topics=k,\n",
    "        random_state=RANDOM_STATE,\n",
    "        passes=PASSES,\n",
    "        alpha=ALPHA,\n",
    "        eta=ETA,\n",
    "        minimum_probability=0.0\n",
    "    )\n",
    "    perplexity = np.exp2(-model.log_perplexity(corpus))\n",
    "    cm_cv = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "    cv = cm_cv.get_coherence()\n",
    "    cm_umass = CoherenceModel(model=model, corpus=corpus, dictionary=dictionary, coherence='u_mass')\n",
    "    umass = cm_umass.get_coherence()\n",
    "    return model, cv, umass, perplexity\n",
    "\n",
    "results = []\n",
    "models = {}\n",
    "\n",
    "for k in K_RANGE:\n",
    "    mdl, cv, umass, pplx = fit_lda(k)\n",
    "    models[k] = mdl\n",
    "    results.append({\"k\": k, \"coherence_cv\": cv, \"coherence_umass\": umass, \"perplexity\": pplx})\n",
    "    print(f\"k={k:>2}  Cv={cv:.3f}  UMass={umass:.3f}  Perplexity={pplx:.1f}\")\n",
    "\n",
    "res_df = pd.DataFrame(results)\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(7,4))\n",
    "ax1.plot(res_df[\"k\"], res_df[\"coherence_cv\"], marker=\"o\", label=\"Coherence (Cv)\")\n",
    "ax1.plot(res_df[\"k\"], res_df[\"coherence_umass\"], marker=\"o\", linestyle=\"--\", label=\"Coherence (UMass)\")\n",
    "ax1.set_xlabel(\"Number of topics (k)\")\n",
    "ax1.set_ylabel(\"Coherence (higher is better)\")\n",
    "ax1.legend(loc=\"upper left\")\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(res_df[\"k\"], res_df[\"perplexity\"], marker=\"s\", color=\"gray\", alpha=0.5, label=\"Perplexity\")\n",
    "ax2.set_ylabel(\"Perplexity (lower is better)\")\n",
    "plt.title(\"LDA model selection\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "best_k = int(res_df.loc[res_df[\"coherence_cv\"].idxmax(), \"k\"])\n",
    "print(f\"\\nSuggested k by Cv peak: {best_k}\")\n",
    "\n",
    "final_model = models[best_k]  # reuse trained model at best_k\n",
    "\n",
    "# Topic prevalence (mean theta across docs)\n",
    "doc_topic = np.array([\n",
    "    [p for _, p in final_model.get_document_topics(bow, minimum_probability=0.0)]\n",
    "    for bow in corpus\n",
    "])\n",
    "prevalence = doc_topic.mean(axis=0)\n",
    "\n",
    "# Coherence per topic\n",
    "cm_final = CoherenceModel(model=final_model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "coh_per_topic = cm_final.get_coherence_per_topic()\n",
    "overall_cv = cm_final.get_coherence()\n",
    "\n",
    "# Build a tidy table\n",
    "def top_terms(model, topic_id, topn=8):\n",
    "    return \", \".join([w for w, _ in model.show_topic(topic_id, topn=topn)])\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    \"topic_id\": range(best_k),\n",
    "    \"top_terms\": [top_terms(final_model, k, topn=8) for k in range(best_k)],\n",
    "    \"prevalence\": [prevalence[k] for k in range(best_k)],\n",
    "    \"coherence_Cv\": coh_per_topic\n",
    "}).sort_values(\"prevalence\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "summary[\"prevalence\"] = summary[\"prevalence\"].map(lambda x: f\"{x:.3f}\")\n",
    "summary[\"coherence_Cv\"] = summary[\"coherence_Cv\"].map(lambda x: f\"{x:.3f}\")\n",
    "\n",
    "print(f\"\\nFinal model (k={best_k}) overall coherence Cv = {overall_cv:.3f}\")\n",
    "print(summary.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d730bf05-1b55-4cfd-adb2-baf9208e08c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "post_dictionary = corpora.Dictionary(df[\"processed_text_new\"])\n",
    "post_corpus = [post_dictionary.doc2bow(text) for text in df[\"processed_text_new\"]]\n",
    "\n",
    "lda_model_posts = gensim.models.LdaModel(\n",
    "    corpus=post_corpus,\n",
    "    id2word=post_dictionary,\n",
    "    num_topics=6,  # Adjust number of topics as needed\n",
    "    random_state=42,\n",
    "    passes=10\n",
    ")\n",
    "\n",
    "# Display top topics for Reddit posts\n",
    "print(\"\\n\uD83D\uDD39 Top Topics in Youtube Comments:\")\n",
    "for idx, topic in lda_model_posts.print_topics(-1):\n",
    "    print(f\"Topic {idx}: {topic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6959944d-9027-467c-818a-107382590445",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7892804395119502,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "youtube_comments",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}